{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf51547e-96f9-4976-bd27-21eeb1c9e104",
   "metadata": {},
   "source": [
    "### GoogleNet model transfer learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c1dbd2-77bd-41d3-b2f4-d42cf4cfb9d4",
   "metadata": {},
   "source": [
    "GoogleNet, particularly its InceptionV3 model, represents a significant advancement in the field of deep learning and computer vision. Here's a detailed overview:\n",
    "\n",
    "GoogleNet Overview\n",
    "Background: GoogleNet is a deep neural network architecture, first introduced by researchers at Google. The name \"GoogleNet\" is often used interchangeably with \"Inception\", which is the actual name of the architecture.\n",
    "\n",
    "Purpose: It was primarily designed for computer vision tasks, particularly excelling in image classification and detection in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC).\n",
    "\n",
    "Architecture Highlights: The most notable feature of GoogleNet is its deep and complex architecture, which is carefully designed to optimize computational efficiency and reduce the number of parameters (to prevent overfitting).\n",
    "\n",
    "Inception Modules\n",
    "Modular Design: The core idea of GoogleNet is the Inception module. This module performs several convolutions of different sizes (1x1, 3x3, 5x5) and pooling operations in parallel, concatenating their outputs into a single output vector for the next stage.\n",
    "\n",
    "Dimensionality Reduction: To manage computational complexity, 1x1 convolutions are used for dimensionality reduction before larger convolutions.\n",
    "\n",
    "Stacking Modules: Multiple Inception modules are stacked together, allowing the network to learn complex features at various scales.\n",
    "\n",
    "InceptionV3 Specifics\n",
    "Evolution: InceptionV3 is an iteration of the original GoogleNet, incorporating several improvements in terms of architecture and efficiency.\n",
    "\n",
    "Enhancements in V3:\n",
    "\n",
    "Factorized Convolutions: Larger convolutions are factorized into smaller, more manageable operations for computational efficiency.\n",
    "Expanded the Filter Bank: Wider inception modules (using more filters of different sizes) allow the model to represent a broader range of features.\n",
    "Label Smoothing: A regularization technique that prevents the model from becoming too confident about its predictions, improving generalization.\n",
    "Applications: InceptionV3 continues to be widely used in various image recognition and computer vision tasks due to its powerful feature extraction capabilities and efficiency.\n",
    "\n",
    "Performance: It offers one of the best trade-offs between accuracy and computational efficiency in the field of image recognition.\n",
    "\n",
    "Impact\n",
    "State-of-the-Art Results: GoogleNet and its iterations like InceptionV3 achieved state-of-the-art results in many benchmarks and competitions.\n",
    "Influence on Future Designs: The inception module concept influenced many subsequent architectures in deep learning, highlighting the importance of carefully balancing depth, width, and computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20ec121d-074d-459f-bfea-e17b607201b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import gc\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Input, Dense, Flatten, concatenate, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "import keras_tuner as kt\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64928aca-3a4b-4135-ae0a-e817cbc6eee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the global policy to mixed_float16\n",
    "set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2a0180e-7054-473f-8bef-0ca38fcbb13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the script uses the GPU if available and set memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set at program startup\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5559507a-d636-422d-85e9-58704dcbb85a",
   "metadata": {},
   "source": [
    "### Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f5236c0-e349-47b4-9635-c72ef1c87947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train loaded\n",
      "X_val loaded\n",
      "y_train loaded\n",
      "y_val loaded\n"
     ]
    }
   ],
   "source": [
    "# Load your preprocessed data\n",
    "X_train = np.load('X_train-299.npy')\n",
    "print('X_train loaded')\n",
    "X_val = np.load('X_val-299.npy')\n",
    "print('X_val loaded')\n",
    "y_train = np.load('y_train-299.npy')\n",
    "print('y_train loaded')\n",
    "y_val = np.load('y_val-299.npy')\n",
    "print('y_val loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a59448-6c8a-4473-9e7e-4996f16ff9d1",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38126c63-31c8-488d-bdc0-8bda3d83c4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_efficientnet_model(hp):\n",
    "    # Load EfficientNetB0 as base model\n",
    "    base_model = EfficientNetB0(include_top=False, input_tensor=Input(shape=(299, 299, 3)), weights='imagenet')\n",
    "    \n",
    "    # Freeze the base model layers based on the hyperparameter value\n",
    "    # The tuner will decide the number of layers to freeze during hyperparameter optimization\n",
    "    for layer in base_model.layers[:-hp.Int('unfreeze_layers', min_value=0, max_value=len(base_model.layers), step=5)]:\n",
    "        if not isinstance(layer, BatchNormalization):  # It's often advised to keep BatchNormalization layers frozen\n",
    "            layer.trainable = False\n",
    "\n",
    "    # Add custom layers on top of EfficientNetB0\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # Additional custom layers\n",
    "    for i in range(hp.Int('num_additional_conv_blocks', 1, 3)):\n",
    "        x = Conv2D(filters=hp.Int(f'conv_filters_{i}', min_value=32, max_value=128, step=32),\n",
    "                   kernel_size=hp.Choice(f'conv_kernel_size_{i}', values=[3, 5]),\n",
    "                   activation='relu', padding='same',\n",
    "                   kernel_regularizer=l2(hp.Float(f'conv_l2_reg_{i}', min_value=1e-5, max_value=1e-2, step=1e-5)))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "        x = Dropout(hp.Float(f'conv_dropout_rate_{i}', min_value=0.1, max_value=0.5, step=0.1))(x)\n",
    "\n",
    "    x = Dense(hp.Int('dense_units', min_value=32, max_value=256, step=32), activation='relu',\n",
    "              kernel_regularizer=l2(hp.Float('dense_l2_reg', min_value=1e-5, max_value=1e-2, step=1e-5)))(x)\n",
    "    x = Dropout(hp.Float('dense_dropout_rate', min_value=0.1, max_value=0.5, step=0.1))(x)\n",
    "    \n",
    "    # Output layer\n",
    "    predictions = Dense(26, activation='softmax')(x)\n",
    "\n",
    "    # Compile the model\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    model.compile(optimizer=Adam(hp.Choice('learning_rate', values=[1e-3, 1e-4])),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afdcd86b-b95f-4a2c-941f-1ca1b923b3c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'build_googlenet_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Set up the tuner for hyperparameter tuning using Hyperband\u001b[39;00m\n\u001b[1;32m      5\u001b[0m tuner \u001b[38;5;241m=\u001b[39m kt\u001b[38;5;241m.\u001b[39mHyperband(\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mbuild_googlenet_model\u001b[49m, \n\u001b[1;32m      7\u001b[0m     objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m      9\u001b[0m     factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     10\u001b[0m     hyperband_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,  \u001b[38;5;66;03m# Number of times to iterate over the full Hyperband algorithm\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     directory\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgooglenet-model-tuning\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     12\u001b[0m     project_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgooglenet-tuning\u001b[39m\u001b[38;5;124m'\u001b[39m  \n\u001b[1;32m     13\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'build_googlenet_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='min')\n",
    "\n",
    "# Set up the tuner for hyperparameter tuning using Hyperband\n",
    "tuner = kt.Hyperband(\n",
    "    build_googlenet_model, \n",
    "    objective='val_accuracy',\n",
    "    max_epochs=10,\n",
    "    factor=3,\n",
    "    hyperband_iterations=2,  # Number of times to iterate over the full Hyperband algorithm\n",
    "    directory='googlenet-model-tuning', \n",
    "    project_name='googlenet-tuning'  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2050a7-9943-4672-8afa-cbf214c027a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for the best hyperparameters\n",
    "tuner.search(X_train, y_train, epochs=10, validation_data=(X_val, y_val), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b575cce-5bba-4606-9c97-de85328729ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_conv_blocks: 1\n",
      "conv_filters_0: 192\n",
      "conv_kernel_size_0: 3\n",
      "conv_l2_reg: 0.00204\n",
      "conv_dropout_rate_0: 0.1\n",
      "dense_units: 192\n",
      "dense_l2_reg: 0.00443\n",
      "dense_dropout_rate: 0.2\n",
      "learning_rate: 0.0001\n",
      "conv_filters_1: 224\n",
      "conv_kernel_size_1: 3\n",
      "conv_dropout_rate_1: 0.0\n",
      "conv_filters_2: 160\n",
      "conv_kernel_size_2: 5\n",
      "conv_dropout_rate_2: 0.4\n"
     ]
    }
   ],
   "source": [
    "# Get the best hyperparameters\n",
    "best_hp = tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "# Print each hyperparameter and its corresponding best value\n",
    "for hp in best_hp.space:\n",
    "    print(f\"{hp.name}: {best_hp.get(hp.name)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee187443-8964-47d3-8d01-0afa36b89e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all completed trials\n",
    "trials = [t for t in tuner.oracle.trials.values() if t.status == 'COMPLETED']\n",
    "\n",
    "# Prepare data for CSV\n",
    "data_to_save = [[\"Trial Number\", \"Hyperparameters\", \"Validation Accuracy\"]]\n",
    "\n",
    "# Add data from each trial\n",
    "for i, trial in enumerate(trials):\n",
    "    trial_hyperparams = trial.hyperparameters.values\n",
    "    val_accuracy = trial.score  \n",
    "    data_to_save.append([f\"Trial {i+1}\", trial_hyperparams, val_accuracy])\n",
    "\n",
    "# Write to CSV\n",
    "with open('googlenet_hyperparameter_trials.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(data_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bae74db-1a6a-49c1-b93d-688a47165b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-15 00:36:25.152629: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-15 00:36:25.154640: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-15 00:36:25.156585: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-15 00:36:25.158573: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-15 00:36:25.160381: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-15 00:36:25.162103: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-15 00:36:25.163903: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-15 00:36:25.165658: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-15 00:36:25.167356: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-15 00:36:25.169085: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-15 00:36:25.170803: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-15 00:36:25.172562: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-15 00:36:27.389036: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-15 00:36:27.391110: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-15 00:36:27.393074: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-15 00:36:27.395077: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-15 00:36:27.397219: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-15 00:36:27.399105: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-15 00:36:27.400960: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-15 00:36:27.402719: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-15 00:36:27.404677: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-15 00:36:27.406510: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13589 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "2023-11-15 00:36:27.407593: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-15 00:36:27.409424: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13589 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n",
      "2023-11-15 00:36:27.410315: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-15 00:36:27.412056: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 13589 MB memory:  -> device: 2, name: Tesla T4, pci bus id: 0000:00:06.0, compute capability: 7.5\n",
      "2023-11-15 00:36:27.412410: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-15 00:36:27.414209: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 13589 MB memory:  -> device: 3, name: Tesla T4, pci bus id: 0000:00:07.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial number of layers in the base model: 311\n",
      "Total number of layers in the model: 319\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-15 00:37:01.278710: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8900\n",
      "2023-11-15 00:37:10.551020: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f48d8004a70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-11-15 00:37:10.551077: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2023-11-15 00:37:10.551085: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\n",
      "2023-11-15 00:37:10.551091: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): Tesla T4, Compute Capability 7.5\n",
      "2023-11-15 00:37:10.551097: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (3): Tesla T4, Compute Capability 7.5\n",
      "2023-11-15 00:37:10.577893: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-11-15 00:37:10.753890: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195/195 [==============================] - ETA: 0s - loss: 2.7292 - accuracy: 0.6862"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195/195 [==============================] - 95s 255ms/step - loss: 2.7292 - accuracy: 0.6862 - val_loss: 1.6101 - val_accuracy: 0.9863 - lr: 1.0000e-04\n",
      "Epoch 2/10\n",
      "195/195 [==============================] - 45s 230ms/step - loss: 1.6231 - accuracy: 0.9643 - val_loss: 1.4593 - val_accuracy: 0.9876 - lr: 1.0000e-04\n",
      "Epoch 3/10\n",
      "195/195 [==============================] - 45s 231ms/step - loss: 1.4212 - accuracy: 0.9878 - val_loss: 1.3211 - val_accuracy: 0.9941 - lr: 1.0000e-04\n",
      "Epoch 4/10\n",
      "195/195 [==============================] - 45s 231ms/step - loss: 1.2762 - accuracy: 0.9928 - val_loss: 1.3014 - val_accuracy: 0.9609 - lr: 1.0000e-04\n",
      "Epoch 5/10\n",
      "195/195 [==============================] - 45s 231ms/step - loss: 1.1779 - accuracy: 0.9830 - val_loss: 1.0933 - val_accuracy: 0.9876 - lr: 1.0000e-04\n",
      "Epoch 6/10\n",
      "195/195 [==============================] - 45s 231ms/step - loss: 1.0562 - accuracy: 0.9845 - val_loss: 0.9862 - val_accuracy: 0.9824 - lr: 1.0000e-04\n",
      "Epoch 7/10\n",
      "195/195 [==============================] - 45s 231ms/step - loss: 0.9204 - accuracy: 0.9907 - val_loss: 0.8421 - val_accuracy: 0.9948 - lr: 1.0000e-04\n",
      "Epoch 8/10\n",
      "195/195 [==============================] - 45s 231ms/step - loss: 0.7982 - accuracy: 0.9952 - val_loss: 0.7284 - val_accuracy: 0.9974 - lr: 1.0000e-04\n",
      "Epoch 9/10\n",
      "195/195 [==============================] - 45s 231ms/step - loss: 0.6901 - accuracy: 0.9947 - val_loss: 0.6406 - val_accuracy: 0.9928 - lr: 1.0000e-04\n",
      "Epoch 10/10\n",
      "195/195 [==============================] - 45s 230ms/step - loss: 0.6169 - accuracy: 0.9904 - val_loss: 0.5960 - val_accuracy: 0.9824 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "# Learning rate scheduler\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Model checkpoint\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    'googlenet-model.h5',  # Path where to save the model\n",
    "    save_best_only=True, \n",
    "    monitor='val_loss', \n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "# Combine all callbacks\n",
    "callbacks_list = [\n",
    "    LearningRateScheduler(scheduler),\n",
    "    early_stopping,\n",
    "    model_checkpoint\n",
    "]\n",
    "\n",
    "# Train model with best hyperparameters within strategy scope\n",
    "model = build_googlenet_model(best_hp)\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks_list, \n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730d6635-afc5-4801-bd8d-ee2a54b36af7",
   "metadata": {},
   "source": [
    "### Model Metrics save output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "802f7bfa-97be-440a-8824-d01d6fe85ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame({\n",
    "    'Epoch': range(1, len(history.history['loss']) + 1),\n",
    "    'Loss': history.history['loss'],\n",
    "    'Accuracy': history.history['accuracy'],\n",
    "    'Val_Loss': history.history['val_loss'],\n",
    "    'Val_Accuracy': history.history['val_accuracy']\n",
    "})\n",
    "\n",
    "# Save the metrics to a CSV file\n",
    "metrics_df.to_csv('googlenet-metrics.csv', index=False)\n",
    "\n",
    "# Save full model \n",
    "model.save('googlenet-fullmodel-full.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce32ddc-fc22-4a4c-979d-3f1d2c1969fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7530f176-a962-4a13-8e07-0a63c1721450",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0366229-f867-4321-bc1c-e6d6b219b34a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113f1a34-420d-4be3-9b6f-8e26e679fa89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f334fc1-5ebb-4ad3-af18-c43927098c30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fa0eee-0532-4ae7-9fc4-b7cd835d31df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27407d0b-697c-40d0-a866-acc84140c935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81bc74f-01d6-4bea-bf22-cf06aab85e60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

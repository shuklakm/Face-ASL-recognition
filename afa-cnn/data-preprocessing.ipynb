{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3aff7221-bf37-40af-ab71-f8c404e45357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import gc\n",
    "import sys\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aedb2c2-9e00-4d05-9fc4-f0fc31eb1e52",
   "metadata": {},
   "source": [
    "### MobileNet Data Preprocessing: 224x224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41be0b57-9ac4-4078-b322-29f467667a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing label: I - 750 images\n",
      "Processing label: P - 750 images\n",
      "Processing label: N - 750 images\n",
      "Processing label: H - 750 images\n",
      "Processing label: K - 750 images\n",
      "Processing label: C - 750 images\n",
      "Processing label: D - 750 images\n",
      "Processing label: W - 750 images\n",
      "Processing label: X - 750 images\n",
      "Processing label: O - 750 images\n",
      "Processing label: Q - 750 images\n",
      "Processing label: F - 750 images\n",
      "Processing label: J - 750 images\n",
      "Processing label: V - 750 images\n",
      "Processing label: G - 750 images\n",
      "Processing label: U - 750 images\n",
      "Processing label: A - 750 images\n",
      "Processing label: L - 750 images\n",
      "Processing label: B - 750 images\n",
      "Processing label: Z - 750 images\n",
      "Processing label: R - 750 images\n",
      "Processing label: M - 750 images\n",
      "Processing label: E - 750 images\n",
      "Processing label: T - 750 images\n",
      "Processing label: S - 750 images\n",
      "Processing label: Y - 750 images\n",
      "Found 15600 validated image filenames belonging to 26 classes.\n",
      "Found 3900 validated image filenames belonging to 26 classes.\n",
      "Files savedprogress: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Define data augmentation parameters for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    rescale=1./255,  # Normalizing the images\n",
    "    validation_split=0.2  # Set the validation split\n",
    ")\n",
    "\n",
    "# No augmentation for validation data, just rescaling\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Base directory containing all images\n",
    "base_directory = './data/train'\n",
    "\n",
    "# Creating a DataFrame containing file paths and labels\n",
    "data = []\n",
    "for label in os.listdir(base_directory):\n",
    "    label_dir = os.path.join(base_directory, label)\n",
    "    if not os.path.isdir(label_dir) or label.startswith('.'):\n",
    "        continue\n",
    "\n",
    "    image_files = os.listdir(label_dir)  # Refresh the list of image files for the current label\n",
    "\n",
    "    # If there are more than 500 images, randomly select 500\n",
    "    if len(image_files) > 500:\n",
    "        image_files = random.sample(image_files, 750)\n",
    "    \n",
    "    print(f\"Processing label: {label} - {len(image_files)} images\") \n",
    "\n",
    "    for img in image_files:\n",
    "        data.append({'filename': os.path.join(label, img), 'class': label})\n",
    "\n",
    "data_df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# Splitting the data into training and validation sets\n",
    "train_df, val_df = train_test_split(data_df, test_size=0.2, stratify=data_df['class'])\n",
    "\n",
    "# Flow images in batches from dataframe for training and validation\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    directory=base_directory,\n",
    "    x_col='filename',\n",
    "    y_col='class',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_dataframe(\n",
    "    dataframe=val_df,\n",
    "    directory=base_directory,\n",
    "    x_col='filename',\n",
    "    y_col='class',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "def main():\n",
    "    # Initialize lists to store data\n",
    "    X_train, y_train = [], []\n",
    "    X_val, y_val = [], []\n",
    "\n",
    "    # Total number of batches\n",
    "    total_train_batches = train_generator.n // train_generator.batch_size\n",
    "    total_val_batches = val_generator.n // val_generator.batch_size\n",
    "\n",
    "    # Iterate over the train generator and collect images and labels\n",
    "    for i in range(total_train_batches):\n",
    "        imgs, labels = next(train_generator)\n",
    "        X_train.append(imgs)\n",
    "        y_train.append(labels)\n",
    "        progress = ((i + 1) / total_train_batches) * 100  # Calculate progress percentage\n",
    "        print(f\"Training progress: {progress:.2f}%\", end=\"\\r\", flush=True)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    X_train = np.concatenate(X_train, axis=0)\n",
    "    y_train = np.concatenate(y_train, axis=0)\n",
    "\n",
    "    # Repeat the process for the validation generator\n",
    "    for i in range(total_val_batches):\n",
    "        imgs, labels = next(val_generator)\n",
    "        X_val.append(imgs)\n",
    "        y_val.append(labels)\n",
    "        progress = ((i + 1) / total_val_batches) * 100  # Calculate progress percentage\n",
    "        print(f\"Validation progress: {progress:.2f}%\", end=\"\\r\", flush=True)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "\n",
    "    X_val = np.concatenate(X_val, axis=0)\n",
    "    y_val = np.concatenate(y_val, axis=0)\n",
    "\n",
    "    # Save the numpy arrays to files\n",
    "    np.save('X_train-s.npy', X_train)\n",
    "    np.save('X_val-s.npy', X_val)\n",
    "    np.save('y_train-s.npy', y_train)\n",
    "    np.save('y_val-s.npy', y_val)\n",
    "    print('Files saved')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c01561-e2a3-4c0c-85f1-fe0b2e3cfa96",
   "metadata": {},
   "source": [
    "### GoogleNet Preprocessing: 299x299"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4458c595-d01d-4218-8f06-83e47690f7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing label: I - 300 images\n",
      "Processing label: P - 300 images\n",
      "Processing label: N - 300 images\n",
      "Processing label: H - 300 images\n",
      "Processing label: K - 300 images\n",
      "Processing label: C - 300 images\n",
      "Processing label: D - 300 images\n",
      "Processing label: W - 300 images\n",
      "Processing label: X - 300 images\n",
      "Processing label: O - 300 images\n",
      "Processing label: Q - 300 images\n",
      "Processing label: F - 300 images\n",
      "Processing label: J - 300 images\n",
      "Processing label: V - 300 images\n",
      "Processing label: G - 300 images\n",
      "Processing label: U - 300 images\n",
      "Processing label: A - 300 images\n",
      "Processing label: L - 300 images\n",
      "Processing label: B - 300 images\n",
      "Processing label: Z - 300 images\n",
      "Processing label: R - 300 images\n",
      "Processing label: M - 300 images\n",
      "Processing label: E - 300 images\n",
      "Processing label: T - 300 images\n",
      "Processing label: S - 300 images\n",
      "Processing label: Y - 300 images\n",
      "Found 6240 validated image filenames belonging to 26 classes.\n",
      "Found 1560 validated image filenames belonging to 26 classes.\n",
      "Training progress: 63.59%\r"
     ]
    }
   ],
   "source": [
    "# Define data augmentation parameters for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    rescale=1./255,  # Normalizing the images\n",
    "    validation_split=0.2  # Set the validation split\n",
    ")\n",
    "\n",
    "# No augmentation for validation data, just rescaling\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Base directory containing all images\n",
    "base_directory = './data/train'\n",
    "\n",
    "# Creating a DataFrame containing file paths and labels\n",
    "data = []\n",
    "for label in os.listdir(base_directory):\n",
    "    label_dir = os.path.join(base_directory, label)\n",
    "    if not os.path.isdir(label_dir) or label.startswith('.'):\n",
    "        continue\n",
    "\n",
    "    image_files = os.listdir(label_dir)  # Refresh the list of image files for the current label\n",
    "\n",
    "    image_files = random.sample(image_files, 300)\n",
    "    \n",
    "    print(f\"Processing label: {label} - {len(image_files)} images\") \n",
    "\n",
    "    for img in image_files:\n",
    "        data.append({'filename': os.path.join(label, img), 'class': label})\n",
    "\n",
    "data_df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# Splitting the data into training and validation sets\n",
    "train_df, val_df = train_test_split(data_df, test_size=0.2, stratify=data_df['class'])\n",
    "\n",
    "# Flow images in batches from dataframe for training and validation\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    directory=base_directory,\n",
    "    x_col='filename',\n",
    "    y_col='class',\n",
    "    target_size=(299, 299),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_dataframe(\n",
    "    dataframe=val_df,\n",
    "    directory=base_directory,\n",
    "    x_col='filename',\n",
    "    y_col='class',\n",
    "    target_size=(299, 299),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "def main():\n",
    "    # Initialize lists to store data\n",
    "    X_train, y_train = [], []\n",
    "    X_val, y_val = [], []\n",
    "\n",
    "    # Total number of batches\n",
    "    total_train_batches = train_generator.n // train_generator.batch_size\n",
    "    total_val_batches = val_generator.n // val_generator.batch_size\n",
    "\n",
    "    # Iterate over the train generator and collect images and labels\n",
    "    for i in range(total_train_batches):\n",
    "        imgs, labels = next(train_generator)\n",
    "        X_train.append(imgs)\n",
    "        y_train.append(labels)\n",
    "        progress = ((i + 1) / total_train_batches) * 100  # Calculate progress percentage\n",
    "        print(f\"Training progress: {progress:.2f}%\", end=\"\\r\", flush=True)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    X_train = np.concatenate(X_train, axis=0)\n",
    "    y_train = np.concatenate(y_train, axis=0)\n",
    "\n",
    "    # Repeat the process for the validation generator\n",
    "    for i in range(total_val_batches):\n",
    "        imgs, labels = next(val_generator)\n",
    "        X_val.append(imgs)\n",
    "        y_val.append(labels)\n",
    "        progress = ((i + 1) / total_val_batches) * 100  # Calculate progress percentage\n",
    "        print(f\"Validation progress: {progress:.2f}%\", end=\"\\r\", flush=True)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    X_val = np.concatenate(X_val, axis=0)\n",
    "    y_val = np.concatenate(y_val, axis=0)\n",
    "\n",
    "    # Save the numpy arrays to files\n",
    "    np.save('X_train-299.npy', X_train)\n",
    "    np.save('X_val-299.npy', X_val)\n",
    "    np.save('y_train-299.npy', y_train)\n",
    "    np.save('y_val-299.npy', y_val)\n",
    "    \n",
    "    print('Files saved')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9552d10-bd5a-454b-8ff8-3ba527ee4987",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
